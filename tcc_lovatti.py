# -*- coding: utf-8 -*-
"""TCC_Lovatti.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ljz-LCUGXA88j_Lx-7vcTO-Te_8DRcC0

# **1. Coleta e Análise de Dados**

1.1. Mostrar versão do Python
"""

import sys

print("Python Version: ", sys.version)

"""1.2. Importar alguns módulos

"""

import pandas as pd
import numpy as np

"""**1.3. Coleta das Vendas**

1.3.1. Conectar ao google drive e à pasta onde estão os arquivos de vendas que foram extraídos do banco de dados
"""

from google.colab import drive

drive.mount('/content/drive/')

import os

os.chdir('drive/My Drive/TCC_LOVATTI')

"""1.3.2. Listar todos os arquivos CSV de vendas"""

import glob

all_files = glob.glob('*sorvetes*.csv')
all_files.sort()
all_files

"""1.3.3. Ler os dados contidos nos arquivos"""

import pandas as pd
from datetime import datetime

custom_date_parser = lambda x: datetime.strptime(x, '%Y-%m-%d')

def r(f):
  return pd.read_csv(f, sep='|', parse_dates=['VDA_DATA'], date_parser=custom_date_parser)

vendas = pd.concat(map(r, all_files))

vendas.sort_values(by=['VDA_DATA'])

# apresentar informações do arquivo
vendas.info()

# apresentar algumas linhas de dados e os nomes das colunas
vendas.head(5)
#vendas.tail(30)

"""1.3.4. Contar a quantidade de linhas do dataframe"""

# contar a quantidade de linhas
vendas.count()

"""1.3.5. Listar anos de dados"""

vendas['VDA_ANO'] = pd.DatetimeIndex(vendas['VDA_DATA']).year
anos = vendas['VDA_ANO'].unique()
anos

"""1.3.6.Somar quantidade de vendas"""

vendas['VDA_QTD'].sum()

"""1.3.7. Somar quantidade de vendas por ano"""

vendas_ano = vendas.groupby(['VDA_ANO']).sum()
vendas_ano

"""1.3.8. Plotar um gráfico de quantidade de vendas por data"""

import matplotlib.pyplot as plt 

vendas['VDA_ANOMES'] = vendas['VDA_DATA'].dt.strftime('%Y-%m')
vendas.pivot_table(values = 'VDA_QTD', index = 'VDA_ANOMES', aggfunc='sum').plot(kind='bar', figsize=(15, 6))
#plt.tight_layout()
plt.show()

"""1.3.9. Plotar um gráfico de quantidade de vendas por dia da semana"""

vendas['VDA_DIASEMANA'] = vendas['VDA_DATA'].dt.dayofweek.astype(str) +'_'+ vendas['VDA_DATA'].dt.day_name()
vendas.pivot_table(values = 'VDA_QTD', index = 'VDA_DIASEMANA', aggfunc='sum').plot(kind='bar', figsize=(15, 6))
#plt.tight_layout()
plt.show()

"""1.3.10. Plotar um gráfico de quantidade de vendas por semana do ano - semanas agrupadas"""

vendas['VDA_SEMANA'] = vendas['VDA_DATA'].dt.isocalendar().week
vendas["VDA_SEMANA"].replace({53: 52}, inplace=True)
vendas.pivot_table(values = 'VDA_QTD', index = 'VDA_SEMANA', aggfunc='sum').plot(kind='bar', figsize=(15, 6))
#plt.tight_layout()
plt.show()

"""1.3.11. Plotar um gráfico de quantidade de vendas por mês do ano - meses agrupados"""

vendas['VDA_MES'] = vendas['VDA_DATA'].dt.month
vendas.pivot_table(values = 'VDA_QTD', index = 'VDA_MES', aggfunc='sum').plot(kind='bar', figsize=(15, 6))
#plt.tight_layout()
plt.show()

"""1.3.12. Plotar um gráfico de quantidade de vendas por semana do ano - semanas separadas"""

vendas['VDA_SEMANA2'] = vendas['VDA_DATA'].dt.isocalendar().week.astype(str).str.zfill(2)
vendas['VDA_SEMANA2'] = vendas['VDA_DATA'].dt.isocalendar().year.astype(str) +'-'+ vendas['VDA_SEMANA2']
vendas.pivot_table(values = 'VDA_QTD', index = 'VDA_SEMANA2', aggfunc='sum').sort_values(('VDA_SEMANA2'), ascending=True).plot(kind='bar', figsize=(20, 6))
#plt.tight_layout()
plt.show()

"""**1.4. Coleta dos Dados Meteorológicos**

1.4.1. Download dados meteorologicos do INMET

Necessário:
- baixar os arquivos zip para o drive
- ler o arquivo com o nome correto da cidade de Vitória (não está 100% padronizado entre os arquivos)
- converter para um dataframe (o formato de data não está 100% padronizado entre os arquivos)
"""

from datetime import datetime

custom_date_parser_1 = lambda x: datetime.strptime(x, '%Y-%m-%d')
custom_date_parser_2 = lambda x: datetime.strptime(x, '%Y/%m/%d')

import requests
import zipfile

for ano in range(2018, 2023):

  file_url = f"https://portal.inmet.gov.br/uploads/dadoshistoricos/{ano}.zip"
  file_gdrive = f"INMET-{ano}.zip" 

  if not os.path.exists(file_gdrive):
    r = requests.get(file_url, stream = True)

    with open(file_gdrive, "wb") as file:
      for block in r.iter_content(chunk_size = 1024):
        if block:
          file.write(block)

  with zipfile.ZipFile(file_gdrive) as myzip:
    # print(myzip.namelist())
    if ano in [2018, 2019]:
      file_csv = f"{ano}/INMET_SE_ES_A612_VITORIA_01-01-{ano}_A_31-12-{ano}.CSV"
    elif ano in [2022]:
      file_csv = f"INMET_SE_ES_A612_VITORIA_01-01-{ano}_A_28-02-{ano}.CSV"
    else:
      file_csv = f"INMET_SE_ES_A612_VITORIA_01-01-{ano}_A_31-12-{ano}.CSV"
    print(file_csv)  

    with myzip.open(file_csv) as myfile:
        var_name = 'df_INMET_{}'.format(str(ano))
        if ano in [2018]:
          col = 'DATA (YYYY-MM-DD)'
          parser = custom_date_parser_1
          globals()[var_name] = pd.read_csv(myfile, sep=";", encoding = "ISO-8859-1", header=8, parse_dates=[col], date_parser=parser, decimal=",")
          globals()[var_name] = globals()[var_name][['DATA (YYYY-MM-DD)', 'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)']].rename(columns={"DATA (YYYY-MM-DD)": "ds", "PRECIPITAÇÃO TOTAL, HORÁRIO (mm)": "precipitacao", "TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)": "temperatura"})
        else:  
          col = 'Data'
          parser = custom_date_parser_2
          globals()[var_name] = pd.read_csv(myfile, sep=";", encoding = "ISO-8859-1", header=8, parse_dates=[col], date_parser=parser, decimal=",")
          globals()[var_name] = globals()[var_name][['Data', 'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)']].rename(columns={"Data": "ds", "PRECIPITAÇÃO TOTAL, HORÁRIO (mm)": "precipitacao", "TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)": "temperatura"})
        

dados_meteor_all = pd.concat([globals()["df_INMET_2018"],globals()["df_INMET_2019"],globals()["df_INMET_2020"],globals()["df_INMET_2021"],globals()["df_INMET_2022"]])
dados_meteor_all

"""1.4.2. Agrupar por data e fazer a média das colunas precipitação e temperatura"""

# Total de linhas da tabela de dados meteorológicos
#dados_meteor.head(30)
dados_meteor_all.count()

# tratamento de valores nulos
dados_meteor_all = dados_meteor_all.fillna({'precipitacao':0})
dados_meteor_all = dados_meteor_all.fillna({'temperatura':0})
# remoção de ocorrências com valores iguais ou inferiores a 0
dados_meteor_all = dados_meteor_all[dados_meteor_all['temperatura'] > 0]

dados_meteor = pd.pivot_table(
   dados_meteor_all,
   index=['ds'],
   aggfunc={'precipitacao': np.sum, 'temperatura': np.mean}
)

dados_meteor.head(30)

"""1.4.3. Plotar um gráfico com a média de temperatura por data"""

dados_meteor_temp = pd.pivot_table(
   dados_meteor_all,
   index=['ds'],
   aggfunc={'temperatura': np.mean}
)
dados_meteor_temp.plot(kind='line', figsize=(15, 6))
plt.show()

"""1.4.4. Plotar um gráfico de quantidade de vendas por mês do ano - meses agrupados"""

dados_meteor_temp = pd.pivot_table(
   dados_meteor_all,
   index=['ds'],
   aggfunc={'precipitacao': np.sum}
)
dados_meteor_temp.plot(kind='line', figsize=(15, 6))
plt.show()

"""# **2. Forecasting com SKTIME**

**sktime**

sktime fornece um framework de código aberto fácil de usar, flexível e modular para uma ampla variedade de tarefas de aprendizagem de máquina de séries temporais. Ele oferece interfaces compatíveis com scikit-learn e ferramentas de composição de modelos, com o objetivo de tornar o ecossistema mais utilizável e interoperável como um todo.

2.1. Importar módulos do sktime com todos os extras
"""

pip install sktime[all_extras]

"""2.2. Agrupar as vendas por ano e mês no formato YYYY-MM e apresentar um gráfico com as partes de treinamento e teste"""

import warnings
warnings.simplefilter("ignore")

from sktime.forecasting.model_selection import temporal_train_test_split
#from sktime.utils.plotting.forecasting import plot_ys
from sktime.utils.plotting import plot_series

vendas_anomes = vendas[['VDA_ANOMES', 'VDA_QTD']]
vendas_anomes['VDA_AM'] = pd.to_datetime(vendas['VDA_ANOMES'], format='%Y-%m')
vendas_anomes

vendas_anomes = vendas_anomes.groupby(['VDA_AM', 'VDA_ANOMES']).sum().reset_index()
vendas_anomes = vendas_anomes[['VDA_AM', 'VDA_ANOMES', 'VDA_QTD']]
vendas_anomes


# adicionar linhas inexistentes
for ano in range(2018, 2023):
  for mes in range(1, 13):
    if ano == 2022 and mes > 3:
      break
    ano_mes = str(ano)+'-'+str(mes).zfill(2)
    #print(ano_mes)
    i = vendas_anomes.query("VDA_ANOMES == '"+ano_mes+"'")
    if i.empty:
      #print(ano_mes)
      vendas_anomes = vendas_anomes.append({'VDA_AM': pd.to_datetime(ano_mes, format='%Y-%m'), 'VDA_ANOMES': ano_mes, 'VDA_QTD': 0}, ignore_index=True)

index = pd.period_range(start='2018-01-01', end='2022-03-01', freq='M')
index

y = pd.Series(vendas_anomes['VDA_QTD'].values, index=index)
y = y.sort_index() # This is an important step if 'time' field is not sorted

#y.head()

y_train, y_test = temporal_train_test_split(y, test_size=0.1) #default 0.25 = 25%
plot_series(y_train, y_test, labels=["y_train", "y_test"]);

"""2.3. Apresentar um gráfico com as partes de treinamento e teste sem agrupar as vendas"""

# vendas_datas = vendas[['VDA_DATA', 'VDA_QTD']]

# y = pd.Series(vendas_datas['VDA_QTD'].values, index=vendas_datas['VDA_DATA'].values)
# y = y.sort_index() # This is an important step if 'time' field is not sorted

# y_train, y_test = temporal_train_test_split(y, test_size=0.1) #default 0.25 = 25%
# plot_series(y_train, y_test, labels=["y_train", "y_test"])

y

"""2.4. Algumas importações do SKTIME"""

# usar mesmas métricas que visualizaremos também mais à frente no outro algoritmo Facebook Prophet
# mse 	rmse 	mae 	mape 	mdape
# https://www.sktime.org/en/stable/api_reference/performance_metrics.html

# Mean squared error (MSE) or root mean squared error (RMSE).
from sktime.performance_metrics.forecasting import MeanSquaredError

# MAE Mean absolute error (MAE).
from sktime.performance_metrics.forecasting import MeanAbsoluteError
# MAPE Mean absolute percentage error (MAPE) or symmetric version.
from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError
# Median absolute percentage error (MdAPE) or symmetric version.
from sktime.performance_metrics.forecasting import MedianAbsolutePercentageError

from sktime.forecasting.model_evaluation import evaluate
from sktime.forecasting.model_selection import ExpandingWindowSplitter

mse = MeanSquaredError(square_root=False)
rmse = MeanSquaredError(square_root=True)
mae = MeanAbsoluteError()
mape = MeanAbsolutePercentageError(symmetric=False)
mdape = MedianAbsolutePercentageError(symmetric=False)

#fh = ForecastingHorizon(y_test.index, is_relative=False,)
#fh = np.array([1, 2, 3, 4, 5]) 
fh = np.arange(len(y_test)) + 1
fh

"""2.4. Modelo NaiveForecaster"""

from sktime.forecasting.naive import NaiveForecaster
from sktime.forecasting.base import ForecastingHorizon

# Naive Forecaster com estratégia ''last''
naive_forecaster_last = NaiveForecaster(strategy="last")
naive_forecaster_last.fit(y_train)
y_last = naive_forecaster_last.predict(fh)

# Naive Forecaster com estratégia ''last'' e sp
naive_forecaster_seasonal_last = NaiveForecaster(strategy="last", sp = 12)
naive_forecaster_seasonal_last.fit(y_train)
y_seasonal_last = naive_forecaster_seasonal_last.predict(fh)

# Naive Forecaster com estratégia ''mean''
naive_forecaster_mean = NaiveForecaster(strategy="mean", sp=12)
naive_forecaster_mean.fit(y_train)
y_mean = naive_forecaster_mean.predict(fh)

# Naive Forecaster com estratégia ''drift''
naive_forecaster_drift = NaiveForecaster(strategy="drift")
naive_forecaster_drift.fit(y_train)
y_drift = naive_forecaster_drift.predict(fh)

plot_series(y_train, y_test, y_last, y_seasonal_last, y_mean, y_drift, labels=["y_train", "y_test", "y_pred_last", "y_pred_seasonal_last", "y_pred_mean", "y_pred_drift"]);

"""MSE, RMSE, MAE, MAPE, MDAPE - NaiveForecaster"""

print('mse / rmse / mae / mape / mdape')

print(*['Estratégia last: ', 'mse =', mse(y_test, y_last), 'rmse =', rmse(y_test, y_last), 'mae =', mae(y_test, y_last), 'mape =', mape(y_test, y_last), 'mdape =', mdape(y_test, y_last)])
print(*['Estratégia seasonal last: ', 'mse =', mse(y_test, y_seasonal_last), 'rmse =', rmse(y_test, y_seasonal_last), 'mae =', mae(y_test, y_seasonal_last), 'mape =', mape(y_test, y_seasonal_last), 'mdape =', mdape(y_test, y_seasonal_last)])
print(*['Estratégia mean: ', 'mse =', mse(y_test, y_mean), 'rmse =', rmse(y_test, y_mean), 'mae =', mae(y_test, y_mean), 'mape =', mape(y_test, y_mean), 'mdape =', mdape(y_test, y_mean)])
print(*['Estratégia drift: ', 'mse =', mse(y_test, y_drift), 'rmse =', rmse(y_test, y_drift), 'mae =', mae(y_test, y_drift), 'mape =', mape(y_test, y_drift), 'mdape =', mdape(y_test, y_drift)])

"""2.5. Modelo AUTOARIMA"""

from sktime.forecasting.arima import AutoARIMA

arima_1 = AutoARIMA(sp=12, suppress_warnings=True)
arima_1.fit(y_train)
y_arima_1 = arima_1.predict(fh)

plot_series(y_train, y_test, y_arima_1, labels=["y_train", "y_test", "y_pred_arima_1"]);

"""MSE, RMSE, MAE, MAPE, MDAPE - AUTOARIMA"""

print('mse / rmse / mae / mape / mdape')

print(*['AUTOARIMA: ', 'mse =', mse(y_test, y_arima_1), 'rmse =', rmse(y_test, y_arima_1), 'mae =', mae(y_test, y_arima_1), 'mape =', mape(y_test, y_arima_1), 'mdape =', mdape(y_test, y_arima_1)])

"""2.6. Modelo AUTOETS"""

from sktime.forecasting.ets import AutoETS

ets_1 = AutoETS(auto=True, sp=12, n_jobs=-1)
ets_1.fit(y_train)
y_ets_1 = ets_1.predict(fh)

plot_series(y_train, y_test, y_ets_1, labels=["y_train", "y_test", "y_pred_ets_1"]);

"""MSE, RMSE, MAE, MAPE, MDAPE - AUTOETS"""

print('mse / rmse / mae / mape / mdape')

print(*['AUTOETS: ', 'mse =', mse(y_test, y_ets_1), 'rmse =', rmse(y_test, y_ets_1), 'mae =', mae(y_test, y_ets_1), 'mape =', mape(y_test, y_ets_1), 'mdape =', mdape(y_test, y_ets_1)])

"""2.7. Modelo BATS"""

from sktime.forecasting.bats import BATS
from sktime.forecasting.base import ForecastingHorizon

bats_1 = BATS(sp=12, use_trend=True, use_box_cox=False)
bats_1.fit(y_train)
y_bats_1 = bats_1.predict(fh)

plot_series(y_train, y_test, y_bats_1, labels=["y_train", "y_test", "y_pred_bats_1"])

"""MSE, RMSE, MAE, MAPE, MDAPE - BATS"""

print('mse / rmse / mae / mape / mdape')

print(*['BATS: ', 'mse =', mse(y_test, y_bats_1), 'rmse =', rmse(y_test, y_bats_1), 'mae =', mae(y_test, y_bats_1), 'mape =', mape(y_test, y_bats_1), 'mdape =', mdape(y_test, y_bats_1)])

"""2.8. Modelo Exponential Smoothing"""

from sktime.forecasting.exp_smoothing  import ExponentialSmoothing

trend_1 = ExponentialSmoothing(trend='mul', seasonal='mul', sp=6)
trend_1.fit(y_train)
y_trend_1 = trend_1.predict(fh)

plot_series(y_train, y_test, y_trend_1, labels=["y_train", "y_test", "y_pred_trend_1"]);

"""MSE, RMSE, MAE, MAPE, MDAPE - Exponential Smoothing"""

print('mse / rmse / mae / mape / mdape')

print(*['Exponential Smoothing: ', 'mse =', mse(y_test, y_trend_1), 'rmse =', rmse(y_test, y_trend_1), 'mae =', mae(y_test, y_trend_1), 'mape =', mape(y_test, y_trend_1), 'mdape =', mdape(y_test, y_trend_1)])

"""2.9. Modelo Prophet"""

from sktime.forecasting.fbprophet import Prophet
from sktime.forecasting.base import ForecastingHorizon

y2_train, y2_test = temporal_train_test_split(y, test_size=0.1)
fh2 = ForecastingHorizon(y2_test.index, is_relative=False)

# Convert index to pd.DatetimeIndex
z = y.copy()
z = z.to_timestamp(freq="M")
z_train, z_test = temporal_train_test_split(z, test_size=0.1)

prophet_1 = Prophet(
    seasonality_mode="multiplicative",
    n_changepoints=int(len(y2_train) / 12),
    add_country_holidays={"country_name": "Brazil"},
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False,
)

prophet_1.fit(z_train)
y_prophet_1 = prophet_1.predict(fh2.to_relative(cutoff=y2_train.index[-1]))
y_prophet_1.index = y2_test.index

plot_series(y2_train, y2_test, y_prophet_1, labels=["y_train", "y_test", "y_pred_prophet_1"]);

"""MSE, RMSE, MAE, MAPE, MDAPE - Prophet"""

print('mse / rmse / mae / mape / mdape')

print(*['Prophet: ', 'mse =', mse(y_test, y_prophet_1), 'rmse =', rmse(y_test, y_prophet_1), 'mae =', mae(y_test, y_prophet_1), 'mape =', mape(y_test, y_prophet_1), 'mdape =', mdape(y_test, y_prophet_1)])

"""# **3. Forecasting com Facebook Prophet e Dados Meteorológicos**

O Prophet é um procedimento para prever dados de séries temporais com base em um modelo aditivo em que as tendências não lineares são ajustadas à sazonalidade anual, semanal e diária, além dos efeitos de feriados. Funciona melhor com séries temporais com fortes efeitos sazonais e várias temporadas de dados históricos. O Prophet é robusto a dados ausentes e mudanças na tendência e normalmente lida bem com valores discrepantes.

O Prophet é um software de código aberto lançado pela equipe Core Data Science do Facebook. Está disponível para download no CRAN e no PyPI.

3.1. Instalar o facebook prophet, se necessário
"""

#pip install pystan==2.19.1.1

!pip install prophet

from prophet import Prophet
from prophet.diagnostics import cross_validation
from prophet.diagnostics import performance_metrics

"""3.2. Merge dos dados meteorológicos com o dataframe de vendas"""

df_p2 = vendas[['VDA_DATA', 'VDA_QTD']].rename(columns={"VDA_DATA": "ds", "VDA_QTD": "y"}).sort_values(by=['ds'])

df_p2 = df_p2[df_p2['ds'] <= '2022-02-28']

df_p2 = df_p2.merge(dados_meteor, on='ds', how='left').fillna({'y':0})

df_p2 = df_p2.fillna({'precipitacao':0})
df_p2 = df_p2.fillna({'temperatura':0})

df_p2.loc[df_p2['temperatura'] == np.NaN, 'sensacao'] = 0 #'indisponivel'
df_p2.loc[df_p2['temperatura'] <= 0, 'sensacao'] = 0 #'indisponivel'
df_p2.loc[df_p2['temperatura'] > 0, 'sensacao'] = 1 # 'frio'
df_p2.loc[df_p2['temperatura'] >= 20, 'sensacao'] = 2 # 'agradavel'
df_p2.loc[df_p2['temperatura'] >= 30, 'sensacao'] = 3 # 'quente'
df_p2.loc[df_p2['temperatura'] >= 35, 'sensacao'] = 4 # 'muito quente'

df_p2.style

"""3.3. Procurar Melhores Parâmetros"""

import itertools

# usado no Hyperparameter tuning
df_p0 = df_p2

param_grid = {  
    'changepoint_prior_scale': [ 0.05, 0.07, 0.08, 0.1],
    #'holidays_prior_scale'   : [0.01,  0.1,   5.0,  10.0],
    'seasonality_prior_scale': [0.05,  0.1,  0.5, 5.0, 7.0, 10.0]
    #'seasonality_mode'       : ['multiplicative', 'additive']
}

# Generate all combinations of parameters
all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
rmses = []  # Store the RMSEs for each params here
maes = []
mapes = []

index = 0
# Use cross validation to evaluate all parameters
for params in all_params:
    index += 1
    print(index)
    p0 = Prophet(
                growth='linear',
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False,
                holidays=None,
                seasonality_mode='multiplicative',
                #seasonality_prior_scale=5,
                #holidays_prior_scale=10,
                #changepoint_prior_scale=.001,
                mcmc_samples=0,        
                **params
                ).add_seasonality(name='monthly',
                                  period=30.5,
                                  fourier_order=3,
                                  #prior_scale=10,
                                  mode='additive')
                
    # .add_seasonality(name='yearly',
    #                                period=365.25,
    #                                fourier_order=5,
    #                                #prior_scale=0.1,
    #                                mode='additive')
                
                                   
    #p0.add_country_holidays(country_name='Brazil')
    p0.add_regressor('precipitacao')
    p0.add_regressor('temperatura')
    p0.add_regressor('sensacao')
    p0.fit(df_p0)  # Fit model with given params
    #df_cv_p0 = cross_validation(p0, cutoffs=cutoffs, horizon='90 days', parallel="processes")
    #df_cv_p0 = cross_validation(p0, initial='1460 days', period='365 days', horizon = '30 days', parallel="processes")
    df_cv_p0 = cross_validation(p0, period='365 days', horizon = '30 days', parallel="processes")
    df_p = performance_metrics(df_cv_p0, rolling_window=0.2)
    rmses.append(df_p['rmse'].values[0])
    maes.append(df_p['mae'].values[0])
    mapes.append(df_p['mape'].values[0])

# Find the best parameters
tuning_results = pd.DataFrame(all_params)
tuning_results['rmse'] = rmses
tuning_results['mae'] = maes
tuning_results['mape'] = mapes

"""3.4. Apresentação das análises com diferentes parametrizações"""

#tuning_results.head(150)
#display(tuning_results)

tuning_results.style

"""3.5. Fit"""

p2 = Prophet(growth='linear',
            yearly_seasonality=True,
            weekly_seasonality=True,
            daily_seasonality=False,
            holidays=None,
            seasonality_mode='multiplicative',
            seasonality_prior_scale=0.5,
            #holidays_prior_scale=5,
            changepoint_prior_scale=.05,
            mcmc_samples=0
            ).add_seasonality(name='monthly',
                                  period=30.5,
                                  fourier_order=3,
                                  #prior_scale=10,
                                  mode='additive')
            
            # .add_seasonality(name='yearly',
            #                        period=365.25,
            #                        fourier_order=3,
            #                        #prior_scale=0.1,
            #                        mode='multiplicative')

                 
#p2.add_country_holidays(country_name='Brazil')
p2.add_regressor('precipitacao')
p2.add_regressor('temperatura')
p2.add_regressor('sensacao')

p2.fit(df_p2)

"""3.6. Previsão de vendas"""

future_p2 = df_p2

forecast_p2 = p2.predict(future_p2)
forecast_p2[forecast_p2['ds'] >= '2022-01-01']

"""3.7. Plotar um gráfico com a previsão de vendas"""

fig1_p2 = p2.plot(forecast_p2, xlabel='Data', ylabel='Vendas')

"""3.8. Plotar um gráfico com os componentes da previsão de vendas"""

fig2_p2 = p2.plot_components(forecast_p2)

"""3.9. Validação e métricas de performance"""

from fbprophet.diagnostics import cross_validation
df_cv_p2 = cross_validation(p2, period='365 days', horizon = '30 days', parallel="processes")
df_cv_p2.style

from fbprophet.diagnostics import performance_metrics
df_perf_p2 = performance_metrics(df_cv_p2)
df_perf_p2.style

"""4.0. Plotar um gráfico com a métrica de performance MAE e MAPE"""

from fbprophet.plot import plot_cross_validation_metric
fig3_p2 = plot_cross_validation_metric(df_cv_p2, metric='mape')

"""4.1. Plotar um gráfico de comparação entre previsto e real nos períodos analisados"""

pht_y = pd.Series(df_cv_p2['y'].values, index=df_cv_p2['ds'].values)
pht_y = pht_y.sort_index() # This is an important step if 'time' field is not sorted

pht_y_hat = pd.Series(df_cv_p2['yhat'].values, index=df_cv_p2['ds'].values)
pht_y_hat = pht_y_hat.sort_index() # This is an important step if 'time' field is not sorted

plot_series(pht_y, pht_y_hat, labels=["y", "y_hat"])

"""4.2 Plotar um gráfico de comparação entre o valor real e os valores previstos possíveis para mínimo e máximo, nos períodos analisados"""

pht_y = pd.Series(df_cv_p2['y'].values, index=df_cv_p2['ds'].values)
pht_y = pht_y.sort_index() # This is an important step if 'time' field is not sorted

pht_y_hat_lower = pd.Series(df_cv_p2['yhat_lower'].values, index=df_cv_p2['ds'].values)
pht_y_hat_lower = pht_y_hat_lower.sort_index() # This is an important step if 'time' field is not sorted

pht_y_hat_upper = pd.Series(df_cv_p2['yhat_upper'].values, index=df_cv_p2['ds'].values)
pht_y_hat_upper = pht_y_hat_upper.sort_index() # This is an important step if 'time' field is not sorted

plot_series(pht_y, pht_y_hat_lower, pht_y_hat_upper, labels=["y", "y_hat_lower", "y_hat_upper"])